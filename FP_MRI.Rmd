---
title: "STA437 Final Project"
date: "2025-03-26"
output: html_document
---

# {.tabset}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
library(kableExtra)
library(tidyverse)
library(lme4)        # For mixed-effects models
library(ggplot2)     # For visualization
library(reshape2)    # For matrix reshaping
library(igraph)      # For network analysis
library(corrplot)    # For correlation visualization
library(ggrepel)  # For avoiding label overlap
library(MASS)     # For classical MDS
library(CCA)       # For canonical correlation
library(yacca)     # Alternative CCA package (optional)
library(CCP)
library(abind)
library(pls)
library(PMA)
library(caret)     # For classification and performance metrics
library(e1071)

load("~/Desktop/Fourth Year/STA437/ABIDE_YALE.RData")


dem_mri <- YALE_demo_var

dem_mri$SubjectID <- 1:nrow(dem_mri)
str(dem_mri)
dem_mri$sexnum <- as.numeric(dem_mri$SEX)
```

## Demographic table

```{r}
# Compute summary by DX_GROUP
demographic_table <- dem_mri %>%
  group_by(DX_GROUP) %>%
  summarise(
    N = n(),
    `Mean Age (SD)` = sprintf("%.2f (%.2f)", mean(AGE_AT_SCAN, na.rm = TRUE), sd(AGE_AT_SCAN, na.rm = TRUE)),
    `Male (%)` = sprintf("%d (%.1f%%)", sum(SEX == "1"), 100 * mean(SEX == "1")),
    `Female (%)` = sprintf("%d (%.1f%%)", sum(SEX == "2"), 100 * mean(SEX == "2"))
  ) %>%
  rename(`Diagnosis Group` = DX_GROUP)

# Compute overall summary
overall_summary <- dem_mri %>%
  summarise(
    `Diagnosis Group` = "Overall",
    N = n(),
    `Mean Age (SD)` = sprintf("%.2f (%.2f)", mean(AGE_AT_SCAN, na.rm = TRUE), sd(AGE_AT_SCAN, na.rm = TRUE)),
    `Male (%)` = sprintf("%d (%.1f%%)", sum(SEX == "1"), 100 * mean(SEX == "1")),
    `Female (%)` = sprintf("%d (%.1f%%)", sum(SEX == "2"), 100 * mean(SEX == "2"))
  )

# Combine overall row with the grouped summary
final_table <- bind_rows(demographic_table, overall_summary)

# Print table using kable
kable(final_table, caption = "Demographic Characteristics", align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```


## PCA

```{r}
# Extract subject-level region-mean data 
region_mean_data <- sapply(YALE_fmri, function(mat) colMeans(mat, na.rm = TRUE))  # Compute mean for each region

# Transpose to have subjects as rows and regions as columns
region_mean_data <- t(region_mean_data)

# Standardize data (center and scale)
scaled_data <- scale(region_mean_data)

# Run PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

# Print summary
summary(pca_result)



# Create a scree plot
scree_data <- data.frame(PC = seq_along(pca_result$sdev), 
                         Variance = (pca_result$sdev^2) / sum(pca_result$sdev^2))

ggplot(scree_data, aes(x = PC, y = Variance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(y = cumsum(Variance)), color = "red", group = 1) +
  labs(title = "Scree Plot", x = "Principal Components", y = "Proportion of Variance Explained") +
  theme_minimal()

# Extract loadings
loadings <- as.data.frame(pca_result$rotation)

# View top contributing regions for PC1 and PC2
top_PC1 <- loadings[order(abs(loadings$PC1), decreasing = TRUE), ][1:10, ]
top_PC2 <- loadings[order(abs(loadings$PC2), decreasing = TRUE), ][1:10, ]

print(top_PC1)
print(top_PC2)


# Create a dataframe for plotting
pca_scores <- data.frame(SubjectID = 1:nrow(region_mean_data),
                         PC1 = pca_result$x[,1],
                         PC2 = pca_result$x[,2],
                         DX_GROUP = dem_mri$DX_GROUP)

rownames(region_mean_data) <- names(YALE_fmri)  # Ensure subject IDs are correctly assigned
pca_scores <- data.frame(SubjectID = 1:nrow(region_mean_data),
                         PC1 = pca_result$x[,1],
                         PC2 = pca_result$x[,2])

# Check if the merge works
pca_scores <- merge(pca_scores, dem_mri[, c("SubjectID", "DX_GROUP")], by = "SubjectID", all.x = TRUE)

# Scatter plot of PC1 vs. PC2
ggplot(pca_scores, aes(x = PC1, y = PC2, color = as.factor(DX_GROUP))) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA: PC1 vs. PC2", x = "PC1", y = "PC2", color = "Group") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue"), labels = c("Autism", "Control"))

```

## MDS

```{r}
# Compute pairwise distances (Euclidean)
dist_matrix <- dist(scaled_data)  # Using pre-scaled data from PCA step

# Run MDS (classical/Kruskal's non-metric)
mds_result <- cmdscale(dist_matrix, k = 2, eig = TRUE)  # k=2 for 2D visualization

mds_scores <- data.frame(
  SubjectID = 1:nrow(region_mean_data),  # Replace with actual IDs if needed
  MDS1 = mds_result$points[, 1],
  MDS2 = mds_result$points[, 2],
  DX_GROUP = dem_mri$DX_GROUP,  # Diagnosis
  SEX = dem_mri$SEX,            # Sex (if available)
  AGE = dem_mri$AGE             # Age (if continuous/categorical)
)

ggplot(mds_scores, aes(x = MDS1, y = MDS2, color = DX_GROUP)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_text_repel(aes(label = SubjectID), size = 3, max.overlaps = 20) +  # Optional labels
  scale_color_manual(values = c("1" = "#F8766D", "2" = "#00BFC4")) +
  labs(
    title = "MDS Plot of fMRI Data (Colored by Diagnosis)",
    x = "MDS Dimension 1",
    y = "MDS Dimension 2",
    color = "Group",
    levels = c("Autism", "Control"),
    breaks = c("Autism", "Control")
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")




```

## CCA

```{r}

# Combine fMRI data
fmri_3d <- abind(YALE_fmri, along=3)  # Creates 196×110×47 array
region_mean_data <- t(apply(fmri_3d, c(2,3), mean))  # 47×110 matrix

age <- dem_mri$AGE_AT_SCAN

# PLS implementation
pls_model <- plsr(age ~ region_mean_data, ncomp=10, scale=TRUE)
X_cca <- pls_model$scores[,1:10]  # Extract scores

# Correct CCA function call
z_data <- dem_mri[, c("AGE_AT_SCAN", "sexnum")]  # Example variables

# Run CCA with multiple z variables
cca_result <- CCA(
  x = scale(X_cca),  # Brain data (47×10)
  z = scale(z_data), # Demographics (47×3)
  typex = "standard",
  typez = "standard",
  penaltyx = 0.3,
  penaltyz = 0.3,
  K = 1
)


perm_results <- CCA.permute(
  x=scale(X_cca),
  z=scale(z_data),
  penaltyxs=seq(0.1,0.5,length=5),
  nperms=1000
)

plot_data <- data.frame(
  Age = age,
  BrainScore = X_cca %*% cca_result$u,
  Group = dem_mri$DX_GROUP
)

ggplot(plot_data, aes(x=Age, y=BrainScore, color=Group)) +
  geom_point(size=3) +
  geom_smooth(method="lm") +
  labs(title=paste("CCA: r =", round(cca_result$cor,2)),
       subtitle=paste("p =", signif(perm_results$pvals[1],3)))
```



## Classification


```{r}
set.seed(437)
# Clean and prepare data
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),  # Use meaningful labels
  region_mean_data
) %>% na.omit()

# Verify factor levels
print(levels(classification_data$Diagnosis))

# Update control to explicitly use Accuracy
ctrl <- trainControl(method = "cv",
                    number = 10,
                    summaryFunction = defaultSummary,  # For Accuracy
                    classProbs = TRUE,
                    savePredictions = TRUE)

### SVM with PCA ###
svm_pipeline <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = c("center", "scale", "pca"),
  trControl = ctrl,
  metric = "Accuracy",  # Force Accuracy metric
  tuneLength = 5
)

# Extract Accuracy properly
svm_accuracy <- svm_pipeline$results %>% 
  dplyr::select(C, Accuracy) %>% 
  filter(!is.na(Accuracy)) %>% 
  arrange(desc(Accuracy)) %>% 
  slice(1) %>% 
  pull(Accuracy)

### LDA with PCA ###
lda_pipeline <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "lda",
  preProcess = c("center", "scale", "pca"),
  trControl = ctrl,
  metric = "Accuracy"
)

lda_accuracy <- lda_pipeline$results$Accuracy[1]

### Visualization ###
results_df <- data.frame(
  Method = c("SVM", "LDA"),
  Accuracy = c(svm_accuracy, lda_accuracy)
)

ggplot(results_df, aes(x = Method, y = Accuracy, fill = Method)) +
  geom_col() +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.5) +
  ylim(0, 1) +
  labs(title = "Classification Accuracy (10-fold CV)",
       subtitle = paste(nrow(classification_data), "samples with",
                       ncol(region_mean_data), "features")) +
  theme_minimal()


# Plot SVM decision boundary (first 2 PCs)
plot_data <- data.frame(
  PC1 = trainPCA[,1],
  PC2 = trainPCA[,2],
  Diagnosis = trainData$Diagnosis
)

ggplot(plot_data, aes(x = PC1, y = PC2, color = Diagnosis)) +
  geom_point() +
  stat_density2d(aes(fill = Diagnosis), alpha = 0.1, geom = "polygon") +
  labs(title = "SVM Decision Regions (First 2 PCs)",
       subtitle = paste("Accuracy:", round(svm_accuracy, 3))) +
  theme_minimal()

# Improved function to handle both models
print_cv_confusion <- function(model) {
  # Get predictions
  if ("bestTune" %in% names(model) && ncol(model$bestTune) > 0) {
    # For models with tuning parameters (like SVM)
    preds <- model$pred %>%
      merge(model$bestTune) %>%  # Select best tune parameters
      arrange(rowIndex)
  } else {
    # For models without tuning (like LDA)
    preds <- model$pred %>%
      arrange(rowIndex)
  }
  
  # Create confusion matrix
  cm <- confusionMatrix(data = preds$pred, 
                       reference = preds$obs)
  
  # Print results
  cat("\n=== Confusion Matrix for", model$method, "===\n")
  print(as.table(cm$table))
  cat("\nClass Statistics:\n")
  print(round(cm$byClass, 3))
  cat("\nOverall Accuracy:", round(cm$overall["Accuracy"], 3), "\n")
}

# Print both matrices
print_cv_confusion(svm_pipeline)
print_cv_confusion(lda_pipeline)
```



```{r}
library(caret)
library(randomForest)
library(tidyverse)

# Prepare data with meaningful class labels
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),
  region_mean_data
) %>% 
  na.omit() 

# Check class distribution
cat("Class distribution:\n")
table(classification_data$Diagnosis)

# Set up control with up-sampling and performance metrics
ctrl <- trainControl(method = "cv",
                    number = 10,
                    sampling = "up",  # Address class imbalance
                    savePredictions = TRUE,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

# Preprocessing steps
preProc <- c("center", "scale", "nzv", "pca")

### 1. SVM with Tuning ###
svm_grid <- expand.grid(
  sigma = c(0.01, 0.1, 1),
  C = c(0.1, 1, 10, 100)  # Regularization parameters
)

svm_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = svm_grid,
  metric = "Sens",  # Focus on sensitivity
  verbose = FALSE
)

### 2. LDA ###
lda_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "lda",
  preProcess = preProc,
  trControl = ctrl,
  metric = "Sens"
)

### 3. Random Forest ###
rf_grid <- expand.grid(
  mtry = c(5, 10, 20),  # Number of features at each split
  splitrule = "gini",
  min.node.size = c(1, 5, 10)
)

rf_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "ranger",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = rf_grid,
  metric = "Sens",
  importance = "permutation"
)

### Performance Evaluation ###
results <- resamples(list(
  SVM = svm_model,
  LDA = lda_model,
  RF = rf_model
))

# Print summary metrics
summary(results)

# Custom confusion matrix function
print_model_results <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  cm <- confusionMatrix(preds$pred, preds$obs)
  
  cat("\n=== ", model$method, "===\n")
  print(cm$table)
  cat("\nAccuracy:", round(cm$overall["Accuracy"], 3))
  cat(" | Sensitivity:", round(cm$byClass["Sensitivity"], 3))
  cat(" | Specificity:", round(cm$byClass["Specificity"], 3), "\n")
  
  # Variable importance if available
  if(!is.null(varImp(model))) {
    cat("\nTop 10 important features:\n")
    print(varImp(model), top = 10)
  }
}

# Print all results
print_model_results(svm_model)
print_model_results(lda_model)
print_model_results(rf_model)

# Visualization
library(ggplot2)

# Performance comparison plot
dotplot(results, metric = "ROC")

# Confusion matrix visualization
plot_confusion <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  ggplot(as.data.frame(confusionMatrix(preds$pred, preds$obs)$table),
         aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "white") +
    scale_fill_gradient(low = "blue", high = "red") +
    ggtitle(paste("Confusion Matrix for", model$method)) +
    theme_minimal()
}

plot_confusion(svm_model)
plot_confusion(lda_model)
plot_confusion(rf_model)
```


```{r}
library(caret)
library(randomForest)
library(tidyverse)

# Prepare data with meaningful class labels
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),
  region_mean_data
) %>% 
  na.omit() %>%
  # Remove near-zero variance features
  dplyr::select(-nearZeroVar(.))

# Check class distribution
cat("Class distribution:\n")
table(classification_data$Diagnosis)

# Set up control with cross-validation
ctrl <- trainControl(method = "cv",
                    number = 10,
                    savePredictions = TRUE,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

# Preprocessing steps (using your 27 PCA components)
preProc <- c("center", "scale", "pca")

### 1. SVM with Simplified Tuning ###
svm_grid <- expand.grid(
  sigma = c(0.01, 0.1, 1),  # Reasonable kernel widths
  C = c(0.1, 1, 10)         # Regularization parameters
)

svm_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = svm_grid,
  metric = "ROC",  # Using AUC which balances sensitivity/specificity
  verbose = FALSE
)

### 2. Random Forest with Simplified Tuning ###
rf_grid <- expand.grid(
  mtry = c(5, 10, 15),      # Number of features at each split
  splitrule = "gini",
  min.node.size = c(1, 5, 10)
)

rf_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "ranger",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = rf_grid,
  metric = "ROC",
  importance = "permutation",
  num.trees = 500  # Slightly more trees than default
)

### 3. Logistic Regression with Elastic Net ###
glmnet_grid <- expand.grid(
  alpha = c(0, 0.5, 1),  # Mix of L1/L2 regularization
  lambda = 10^seq(-3, 0, length = 10)  # Regularization strength
)

glmnet_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "glmnet",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = glmnet_grid,
  metric = "ROC"
)

### Performance Evaluation ###
results <- resamples(list(
  SVM = svm_model,
  RF = rf_model,
  GLMnet = glmnet_model
))

# Print summary metrics
summary(results)

# Custom function to print results
print_model_results <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  cm <- confusionMatrix(preds$pred, preds$obs, positive = "Case")
  
  cat("\n=== ", model$method, "===\n")
  print(cm$table)
  cat("\nAccuracy:", round(cm$overall["Accuracy"], 3))
  cat(" | Sensitivity:", round(cm$byClass["Sensitivity"], 3))
  cat(" | Specificity:", round(cm$byClass["Specificity"], 3))
  cat(" | AUC:", round(max(model$results$ROC), 3), "\n")
}

# Print all results
print_model_results(svm_model)
print_model_results(rf_model)
print_model_results(glmnet_model)

# Visualization
dotplot(results, metric = "ROC")

# Variable importance plots
ggplot(varImp(rf_model)) + ggtitle("Random Forest - Variable Importance")
ggplot(varImp(glmnet_model)) + ggtitle("GLMnet - Variable Importance")
```


```{r}
library(caret)
library(randomForest)
library(tidyverse)
library(doParallel)  # For parallel processing

# Enable parallel processing
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)

# Prepare data with meaningful class labels
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),
  region_mean_data
) %>% 
  na.omit() %>%
  # Remove near-zero variance features
  dplyr::select(-nearZeroVar(.))

# Check class distribution (handle class imbalance)
cat("Class distribution:\n")
table(classification_data$Diagnosis)

# Handle class imbalance using SMOTE or ROSE
if(requireNamespace("DMwR", quietly = TRUE)) {
  classification_data <- DMwR::SMOTE(Diagnosis ~ ., classification_data, perc.over = 200, perc.under = 150)
} else {
  warning("DMwR package not available for SMOTE. Using class weights instead.")
}

# Set up control with repeated cross-validation and stratified sampling
ctrl <- trainControl(method = "repeatedcv",
                    number = 10,
                    repeats = 3,
                    savePredictions = TRUE,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary,
                    sampling = if(exists("classification_data")) "up" else NULL,  # Upsample if SMOTE not used
                    allowParallel = TRUE)

# Enhanced preprocessing
preProc <- c("center", "scale", "pca", "nzv", "corr")  # Added correlation filter

### 1. Enhanced SVM with Wider Tuning ###
svm_grid <- expand.grid(
  sigma = c(0.001, 0.01, 0.1, 1, 10),  # Wider range of kernel widths
  C = 2^seq(-5, 15, length = 20)        # Wider range with exponential scale
)

svm_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = svm_grid,
  metric = "ROC",
  verbose = FALSE
)

### 2. Optimized Random Forest ###
rf_grid <- expand.grid(
  mtry = c(floor(sqrt(ncol(classification_data)-1)),  # Default mtry
  splitrule = c("gini", "extratrees"),               # Additional split rule
  min.node.size = c(1, 3, 5, 10)                     # More node size options
))

rf_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "ranger",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = rf_grid,
  metric = "ROC",
  importance = "permutation",
  num.trees = 1000,  # Increased number of trees
  class.weights = c(Control = 1, Case = 2)  # Handle class imbalance
)

### 3. Improved Logistic Regression with Elastic Net ###
glmnet_grid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # Finer alpha grid
  lambda = 10^seq(-5, 2, length = 50)  # Wider lambda range
)

glmnet_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "glmnet",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = glmnet_grid,
  metric = "ROC",
  family = "binomial"
)

### 4. Add XGBoost as another powerful algorithm ###
if(requireNamespace("xgboost", quietly = TRUE)) {
  xgb_grid <- expand.grid(
    nrounds = c(100, 200),
    max_depth = c(3, 6, 9),
    eta = c(0.01, 0.1, 0.3),
    gamma = c(0, 1),
    colsample_bytree = c(0.6, 0.8, 1),
    min_child_weight = c(1, 5),
    subsample = c(0.8, 1)
  
  xgb_model <- train(
    Diagnosis ~ .,
    data = classification_data,
    method = "xgbTree",
    preProcess = preProc,
    trControl = ctrl,
    tuneGrid = xgb_grid,
    metric = "ROC",
    verbose = FALSE
  )
}

### Performance Evaluation ###
models_list <- if(exists("xgb_model")) {
  list(SVM = svm_model, RF = rf_model, GLMnet = glmnet_model, XGBoost = xgb_model)
} else {
  list(SVM = svm_model, RF = rf_model, GLMnet = glmnet_model)
}

results <- resamples(models_list)

# Print summary metrics
summary(results)

# Enhanced performance reporting function
print_model_results <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  cm <- confusionMatrix(preds$pred, preds$obs, positive = "Case")
  roc_auc <- max(model$results$ROC, na.rm = TRUE)
  
  cat("\n=== ", model$method, "===\n")
  print(cm$table)
  cat("\nAccuracy:", round(cm$overall["Accuracy"], 3))
  cat(" | Sensitivity:", round(cm$byClass["Sensitivity"], 3))
  cat(" | Specificity:", round(cm$byClass["Specificity"], 3))
  cat(" | AUC:", round(roc_auc, 3))
  cat(" | Kappa:", round(cm$overall["Kappa"], 3), "\n")
  
  # Print best tuning parameters
  cat("\nBest parameters:\n")
  print(model$bestTune)
}

# Print all results
sapply(models_list, print_model_results)

# Enhanced visualization
bwplot(results, metric = "ROC", main = "Model Comparison by ROC AUC")
dotplot(results, metric = "ROC")

# Variable importance plots with better formatting
plot(varImp(rf_model), top = 20, main = "Random Forest - Top 20 Important Features")
plot(varImp(glmnet_model), top = 20, main = "GLMnet - Top 20 Important Features")
if(exists("xgb_model")) plot(varImp(xgb_model), top = 20, main = "XGBoost - Top 20 Important Features")

# Stop parallel processing
stopCluster(cl)
```

