---
title: "STA437 Final Project"
date: "2025-03-26"
output: html_document
---

# {.tabset}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
library(kableExtra)
library(tidyverse)
library(lme4)        # For mixed-effects models
library(ggplot2)     # For visualization
library(reshape2)    # For matrix reshaping
library(igraph)      # For network analysis
library(corrplot)    # For correlation visualization
library(ggrepel)  # For avoiding label overlap
library(MASS)     # For classical MDS
library(CCA)       # For canonical correlation
library(yacca)     # Alternative CCA package (optional)
library(CCP)
library(abind)
library(pls)
library(PMA)
library(caret)     # For classification and performance metrics
library(e1071)

load("~/Desktop/Fourth Year/STA437/ABIDE_YALE.RData")


dem_mri <- YALE_demo_var

dem_mri$SubjectID <- 1:nrow(dem_mri)
str(dem_mri)
dem_mri$sexnum <- as.numeric(dem_mri$SEX)
```

## Demographic table

```{r}
# Compute summary by DX_GROUP
demographic_table <- dem_mri %>%
  group_by(DX_GROUP) %>%
  summarise(
    N = n(),
    `Mean Age (SD)` = sprintf("%.2f (%.2f)", mean(AGE_AT_SCAN, na.rm = TRUE), sd(AGE_AT_SCAN, na.rm = TRUE)),
    `Male (%)` = sprintf("%d (%.1f%%)", sum(SEX == "1"), 100 * mean(SEX == "1")),
    `Female (%)` = sprintf("%d (%.1f%%)", sum(SEX == "2"), 100 * mean(SEX == "2"))
  ) %>%
  rename(`Diagnosis Group` = DX_GROUP)

# Compute overall summary
overall_summary <- dem_mri %>%
  summarise(
    `Diagnosis Group` = "Overall",
    N = n(),
    `Mean Age (SD)` = sprintf("%.2f (%.2f)", mean(AGE_AT_SCAN, na.rm = TRUE), sd(AGE_AT_SCAN, na.rm = TRUE)),
    `Male (%)` = sprintf("%d (%.1f%%)", sum(SEX == "1"), 100 * mean(SEX == "1")),
    `Female (%)` = sprintf("%d (%.1f%%)", sum(SEX == "2"), 100 * mean(SEX == "2"))
  )

# Combine overall row with the grouped summary
final_table <- bind_rows(demographic_table, overall_summary)

# Print table using kable
kable(final_table, caption = "Demographic Characteristics", align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```


## PCA

```{r}
# Extract subject-level region-mean data 
region_mean_data <- sapply(YALE_fmri, function(mat) colMeans(mat, na.rm = TRUE))  # Compute mean for each region

# Transpose to have subjects as rows and regions as columns
region_mean_data <- t(region_mean_data)

# Standardize data (center and scale)
scaled_data <- scale(region_mean_data)

# Run PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

# Print summary
summary(pca_result)



# Create a scree plot
scree_data <- data.frame(PC = seq_along(pca_result$sdev), 
                         Variance = (pca_result$sdev^2) / sum(pca_result$sdev^2))

ggplot(scree_data, aes(x = PC, y = Variance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(y = cumsum(Variance)), color = "red", group = 1) +
  labs(title = "Scree Plot", x = "Principal Components", y = "Proportion of Variance Explained") +
  theme_minimal()

# Extract loadings
loadings <- as.data.frame(pca_result$rotation)

# View top contributing regions for PC1 and PC2
top_PC1 <- loadings[order(abs(loadings$PC1), decreasing = TRUE), ][1:10, ]
top_PC2 <- loadings[order(abs(loadings$PC2), decreasing = TRUE), ][1:10, ]

print(top_PC1)
print(top_PC2)


# Create a dataframe for plotting
pca_scores <- data.frame(SubjectID = 1:nrow(region_mean_data),
                         PC1 = pca_result$x[,1],
                         PC2 = pca_result$x[,2],
                         DX_GROUP = dem_mri$DX_GROUP)

rownames(region_mean_data) <- names(YALE_fmri)  # Ensure subject IDs are correctly assigned
pca_scores <- data.frame(SubjectID = 1:nrow(region_mean_data),
                         PC1 = pca_result$x[,1],
                         PC2 = pca_result$x[,2])

# Check if the merge works
pca_scores <- merge(pca_scores, dem_mri[, c("SubjectID", "DX_GROUP")], by = "SubjectID", all.x = TRUE)

# Scatter plot of PC1 vs. PC2
ggplot(pca_scores, aes(x = PC1, y = PC2, color = as.factor(DX_GROUP))) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA: PC1 vs. PC2", x = "PC1", y = "PC2", color = "Group") +
  theme_minimal() +
  scale_color_manual(values = c("red", "blue"), labels = c("Autism", "Control"))

```

## MDS

```{r}
# Compute pairwise distances (Euclidean)
dist_matrix <- dist(scaled_data)  # Using pre-scaled data from PCA step

# Run MDS (classical/Kruskal's non-metric)
mds_result <- cmdscale(dist_matrix, k = 2, eig = TRUE)  # k=2 for 2D visualization

mds_scores <- data.frame(
  SubjectID = 1:nrow(region_mean_data),  # Replace with actual IDs if needed
  MDS1 = mds_result$points[, 1],
  MDS2 = mds_result$points[, 2],
  DX_GROUP = dem_mri$DX_GROUP,  # Diagnosis
  SEX = dem_mri$SEX,            # Sex (if available)
  AGE = dem_mri$AGE             # Age (if continuous/categorical)
)

ggplot(mds_scores, aes(x = MDS1, y = MDS2, color = DX_GROUP)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_text_repel(aes(label = SubjectID), size = 3, max.overlaps = 20) +  # Optional labels
  scale_color_manual(values = c("1" = "#F8766D", "2" = "#00BFC4")) +
  labs(
    title = "MDS Plot of fMRI Data (Colored by Diagnosis)",
    x = "MDS Dimension 1",
    y = "MDS Dimension 2",
    color = "Group",
    levels = c("Autism", "Control"),
    breaks = c("Autism", "Control")
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")




```

## CCA

```{r}

# Combine fMRI data
fmri_3d <- abind(YALE_fmri, along=3)  # Creates 196×110×47 array
region_mean_data <- t(apply(fmri_3d, c(2,3), mean))  # 47×110 matrix

age <- dem_mri$AGE_AT_SCAN

# PLS implementation
pls_model <- plsr(age ~ region_mean_data, ncomp=10, scale=TRUE)
X_cca <- pls_model$scores[,1:10]  # Extract scores

# Correct CCA function call
z_data <- dem_mri[, c("AGE_AT_SCAN", "sexnum")]  # Example variables

# Run CCA with multiple z variables
cca_result <- CCA(
  x = scale(X_cca),  # Brain data (47×10)
  z = scale(z_data), # Demographics (47×3)
  typex = "standard",
  typez = "standard",
  penaltyx = 0.3,
  penaltyz = 0.3,
  K = 1
)


perm_results <- CCA.permute(
  x=scale(X_cca),
  z=scale(z_data),
  penaltyxs=seq(0.1,0.5,length=5),
  nperms=1000
)

plot_data <- data.frame(
  Age = age,
  BrainScore = X_cca %*% cca_result$u,
  Group = dem_mri$DX_GROUP
)

ggplot(plot_data, aes(x=Age, y=BrainScore, color=Group)) +
  geom_point(size=3) +
  geom_smooth(method="lm") +
  labs(title=paste("CCA: r =", round(cca_result$cor,2)),
       subtitle=paste("p =", signif(perm_results$pvals[1],3)))
```



## Classification





```{r}
library(caret)
library(randomForest)
library(tidyverse)

# Prepare data with meaningful class labels
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),
  region_mean_data
) %>% 
  na.omit() 

# Check class distribution
cat("Class distribution:\n")
table(classification_data$Diagnosis)

# Set up control with up-sampling and performance metrics
ctrl <- trainControl(method = "cv",
                    number = 10,
                    sampling = "up",  # Address class imbalance
                    savePredictions = TRUE,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

# Preprocessing steps
preProc <- c("center", "scale", "nzv", "pca")

### 1. SVM with Tuning ###
svm_grid <- expand.grid(
  sigma = c(0.01, 0.1, 1),
  C = c(0.1, 1, 10, 100)  # Regularization parameters
)

svm_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = svm_grid,
  metric = "Sens",  # Focus on sensitivity
  verbose = FALSE
)

### 2. LDA ###
lda_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "lda",
  preProcess = preProc,
  trControl = ctrl,
  metric = "Sens"
)

### 3. Random Forest ###
rf_grid <- expand.grid(
  mtry = c(5, 10, 20),  # Number of features at each split
  splitrule = "gini",
  min.node.size = c(1, 5, 10)
)

rf_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "ranger",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = rf_grid,
  metric = "Sens",
  importance = "permutation"
)

### Performance Evaluation ###
results <- resamples(list(
  SVM = svm_model,
  LDA = lda_model,
  RF = rf_model
))

# Print summary metrics
summary(results)

# Custom confusion matrix function
print_model_results <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  cm <- confusionMatrix(preds$pred, preds$obs)
  
  cat("\n=== ", model$method, "===\n")
  print(cm$table)
  cat("\nAccuracy:", round(cm$overall["Accuracy"], 3))
  cat(" | Sensitivity:", round(cm$byClass["Sensitivity"], 3))
  cat(" | Specificity:", round(cm$byClass["Specificity"], 3), "\n")
  
  # Variable importance if available
  if(!is.null(varImp(model))) {
    cat("\nTop 10 important features:\n")
    print(varImp(model), top = 10)
  }
}

# Print all results
print_model_results(svm_model)
print_model_results(lda_model)
print_model_results(rf_model)

# Visualization
library(ggplot2)

# Performance comparison plot
dotplot(results, metric = "ROC")

# Confusion matrix visualization
plot_confusion <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  ggplot(as.data.frame(confusionMatrix(preds$pred, preds$obs)$table),
         aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "white") +
    scale_fill_gradient(low = "blue", high = "red") +
    ggtitle(paste("Confusion Matrix for", model$method)) +
    theme_minimal()
}

plot_confusion(svm_model)
plot_confusion(lda_model)
plot_confusion(rf_model)
```


```{r}
library(caret)
library(randomForest)
library(tidyverse)

# Prepare data with meaningful class labels
classification_data <- data.frame(
  Diagnosis = factor(dem_mri$DX_GROUP,
                    levels = c(2, 1),
                    labels = c("Control", "Case")),
  region_mean_data
) %>% 
  na.omit() %>%
  # Remove near-zero variance features
  dplyr::select(-nearZeroVar(.))

# Check class distribution
cat("Class distribution:\n")
table(classification_data$Diagnosis)

# Set up control with cross-validation
ctrl <- trainControl(method = "cv",
                    number = 10,
                    savePredictions = TRUE,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

# Preprocessing steps (using your 27 PCA components)
preProc <- c("center", "scale", "pca")

### 1. SVM with Simplified Tuning ###
svm_grid <- expand.grid(
  sigma = c(0.01, 0.1, 1),  # Reasonable kernel widths
  C = c(0.1, 1, 10)         # Regularization parameters
)

svm_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "svmRadial",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = svm_grid,
  metric = "ROC",  # Using AUC which balances sensitivity/specificity
  verbose = FALSE
)

### 2. Random Forest with Simplified Tuning ###
rf_grid <- expand.grid(
  mtry = c(5, 10, 15),      # Number of features at each split
  splitrule = "gini",
  min.node.size = c(1, 5, 10)
)

rf_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "ranger",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = rf_grid,
  metric = "ROC",
  importance = "permutation",
  num.trees = 500  # Slightly more trees than default
)

### 3. Logistic Regression with Elastic Net ###
glmnet_grid <- expand.grid(
  alpha = c(0, 0.5, 1),  # Mix of L1/L2 regularization
  lambda = 10^seq(-3, 0, length = 10)  # Regularization strength
)

glmnet_model <- train(
  Diagnosis ~ .,
  data = classification_data,
  method = "glmnet",
  preProcess = preProc,
  trControl = ctrl,
  tuneGrid = glmnet_grid,
  metric = "ROC"
)

### Performance Evaluation ###
results <- resamples(list(
  SVM = svm_model,
  RF = rf_model,
  GLMnet = glmnet_model
))

# Print summary metrics
summary(results)

# Custom function to print results
print_model_results <- function(model) {
  preds <- model$pred %>% 
    {if("bestTune" %in% names(model)) merge(., model$bestTune) else .} %>%
    arrange(rowIndex)
  
  cm <- confusionMatrix(preds$pred, preds$obs, positive = "Case")
  
  cat("\n=== ", model$method, "===\n")
  print(cm$table)
  cat("\nAccuracy:", round(cm$overall["Accuracy"], 3))
  cat(" | Sensitivity:", round(cm$byClass["Sensitivity"], 3))
  cat(" | Specificity:", round(cm$byClass["Specificity"], 3))
  cat(" | AUC:", round(max(model$results$ROC), 3), "\n")
}

# Print all results
print_model_results(svm_model)
print_model_results(rf_model)
print_model_results(glmnet_model)

# Visualization
dotplot(results, metric = "ROC")

# Variable importance plots
ggplot(varImp(rf_model)) + ggtitle("Random Forest - Variable Importance")
ggplot(varImp(glmnet_model)) + ggtitle("GLMnet - Variable Importance")
```

